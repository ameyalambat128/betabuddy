{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.14","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"none","dataSources":[{"sourceId":5269519,"sourceType":"datasetVersion","datasetId":3053241}],"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# Indoor Climbing Gym Hold Segmentation\n\nFirst, we install the Facebook [detectron2](https://github.com/facebookresearch/detectron2) framework.","metadata":{}},{"cell_type":"code","source":"!git clone https://github.com/facebookresearch/detectron2.git\n%cd detectron2\n!python -m pip install -e ./","metadata":{"execution":{"iopub.status.busy":"2024-10-07T22:34:21.424624Z","iopub.execute_input":"2024-10-07T22:34:21.424973Z","iopub.status.idle":"2024-10-07T22:35:45.329839Z","shell.execute_reply.started":"2024-10-07T22:34:21.424936Z","shell.execute_reply":"2024-10-07T22:35:45.328456Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Using our pre-trained weights, we can segment a sample image.","metadata":{}},{"cell_type":"code","source":"!python -V\n!pip install pandas","metadata":{"execution":{"iopub.status.busy":"2024-10-07T22:48:26.57131Z","iopub.execute_input":"2024-10-07T22:48:26.571837Z","iopub.status.idle":"2024-10-07T22:48:39.412099Z","shell.execute_reply.started":"2024-10-07T22:48:26.571794Z","shell.execute_reply":"2024-10-07T22:48:39.410547Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import os\nimport cv2 as cv\nimport numpy as np\nfrom typing import List\nimport matplotlib.pyplot as plt\nfrom collections import defaultdict\n\nfrom scipy.stats import linregress\nfrom shapely.geometry import Polygon\n\nfrom detectron2.structures import Instances, Boxes\nimport torch\nfrom scipy.stats import kurtosis, skew\n\n\nSTROKE_COLOR = (0, 255, 0)\nSTROKE_THICKNESS = 5\n\nEVALUATION_DATA = [\n    \"../data/bh/0000.jpg\",\n    \"../data/bh/0457.jpg\",\n    \"../data/bh/0518.jpg\",\n    \"../data/bh-phone/126.jpg\",\n    \"../data/bh-phone/182.jpg\",\n    \"../data/sm/082.jpg\",\n    \"../data/sm/108.jpg\",\n]\n\n\ndef dist(p1, p2):\n    \"\"\"Return the Euclidean distance between p1 and p2.\"\"\"\n    return ((p1[0] - p2[0]) ** 2 + (p1[1] - p2[1]) ** 2) ** (1/2)\n\ndef contour_to_list(c):\n    \"\"\"Convert a contour to a list of (x, y) tuples.\"\"\"\n    l = []\n    for j in c:\n        l.append(j.tolist()[0])\n    return l\n\ndef filter_straight_contours(contours, max_avg_error=5):\n    \"\"\"\n    Filter out contours that are too close to lines.\n\n    @param max_avg_error: maximum average squared point-to-line error (in pixels)\n    \"\"\"\n    contours = list(contours)\n\n    def error_function(a, b, c):\n        error = 0\n        for x, y in contour_to_list(c):\n            error += (a * x + b - y) ** 2\n\n        return error / len(c)\n\n    to_remove = []\n    for i, c in enumerate(contours):\n        points = np.asarray(contour_to_list(c))\n\n        x = points[:,0]\n        y = points[:,1]\n\n        result = linregress(x, y)\n\n        a, b = result.slope, result.intercept\n\n        error = error_function(a, b, c)\n\n        if error < max_avg_error:\n            to_remove.append(i)\n\n    for r in reversed(to_remove):\n        del contours[r]\n\n    return contours\n\ndef filter_size_contours(contours, min_points=3, min_bb_area=125):\n    \"\"\"\n    Filter out contours based on the number of their points and their bounding box area.\n\n    @param min_points: the minimum number of points a contour can have\n    @param min_bb_area: the minimum area of a bounding box of a contour\n    \"\"\"\n    contours = list(contours)\n\n    to_remove = []\n    for i, c in enumerate(contours):\n        if len(c) < min_points:\n            to_remove.append(i)\n            continue\n\n        points = [j.tolist()[0] for j in c]\n        p = Polygon(points)\n\n        xl, yl, xh, yh = p.bounds\n\n        w = abs(xl - xh)\n        h = abs(yl - yh)\n\n        if w * h < min_bb_area:\n            to_remove.append(i)\n\n    for r in reversed(to_remove):\n        del contours[r]\n\n    return contours\n\ndef process_image(img, filename, save=True, scaling=0.5):\n    \"\"\"Display or save an image.\"\"\"\n    d = os.path.dirname(filename)\n\n    if save:\n        if not os.path.exists(d):\n            os.mkdir(d)\n\n        h, w = img.shape[:2]\n\n        new_w, new_h = int(w * scaling), int(h * scaling)\n\n        resized_img = cv.resize(img, (new_w, new_h), interpolation= cv.INTER_LINEAR)\n\n        cv.imwrite(filename, resized_img)\n    else:\n        cv.imshow('image', img)\n        cv.waitKey(0)\n\ndef draw_keypoints(img, keypoints, color=STROKE_COLOR, thickness=STROKE_THICKNESS):\n    \"\"\"Custom drawing of keypoints (since the OpenCV function doesn't support custom thickness).\"\"\"\n    for k in keypoints:\n        x, y = k.pt\n        cv.circle(img, (int(x), int(y)), int(k.size / 2), color=color, thickness=thickness)\n\ndef draw_contours(img, contours, color=STROKE_COLOR, thickness=STROKE_THICKNESS):\n    \"\"\"cv.drawContours with sane default.\"\"\"\n    cv.drawContours(img, contours, -1, color=color, thickness=thickness)\n\ndef contour_to_box(contour):\n    min_x, min_y = float('inf'), float('inf')\n    max_x, max_y = 0, 0\n\n    for x, y in contour_to_list(contour):\n        min_x = min(x, min_x)\n        min_y = min(y, min_y)\n        max_x = max(x, max_x)\n        max_y = max(y, max_y)\n\n    return (min_x, min_y, max_x, max_y)\n\ndef contour_to_mask(img, contour):\n    h, w = img.shape[:2]\n    blank = np.zeros(shape=[h, w], dtype=np.uint8)\n\n    cv.fillPoly(blank, contour, color=(255, 255, 255))\n\n    return blank\n\ndef draw_contour_boxes(img, contours, color=STROKE_COLOR, thickness=STROKE_THICKNESS):\n    \"\"\"cv.drawContours with sane default.\"\"\"\n    for c in contours:\n        x1, y1, x2, y2 = contour_to_box(c)\n        cv.rectangle(img, (x1, y1), (x2, y2), color=color, thickness=thickness)\n\ndef gaussian_blur(img, size=13):\n    \"\"\"cv.GaussianBlur with sane default.\"\"\"\n    return cv.GaussianBlur(img, (size, size), 0)\n\ndef canny(img, parameters=(20, 25)):\n    \"\"\"cv.Canny with sane default.\"\"\"\n    return cv.Canny(img, *parameters)\n\ndef find_contours(edges):\n    \"\"\"cv.findContours with sane default.\"\"\"\n    contours, _ = cv.findContours(edges, cv.RETR_EXTERNAL, cv.CHAIN_APPROX_SIMPLE)\n\n    return contours\n\ndef simplify_contours(contours, epsilon=0.005):\n    \"\"\"Simplify contours using the cv.approxPolyDP function.\"\"\"\n    simplified = []\n\n    for c in contours:\n        peri = cv.arcLength(c, True)\n        approx = cv.approxPolyDP(c, epsilon * peri, True)\n        simplified.append(approx)\n\n    return simplified\n\ndef threshold(img, start=0, end=255):\n    \"\"\"cv.threshold with sane default.\"\"\"\n    _, t = cv.threshold(img, start, end, cv.THRESH_BINARY)\n    return t\n\ndef detect_blobs(img):\n    \"\"\"OpenCV simple blob detection with sane default.\"\"\"\n    params = cv.SimpleBlobDetector_Params()\n\n    params.filterByArea = True\n    params.minArea = 500\n    params.maxArea = 1000000\n\n    params.minThreshold = 1\n    params.maxThreshold = 200\n    params.thresholdStep = 10\n\n    params.filterByColor = False\n    params.filterByConvexity = False\n    params.filterByInertia = False\n\n    detector = cv.SimpleBlobDetector_create(params)\n    return detector.detect(img)\n\ndef get_nearby_contours(point, contours, distance):\n    \"\"\"Return a list of contours any of whose points are close enough to the point.\"\"\"\n    def is_close(p, c):\n        for pc in contour_to_list(c):\n            if dist(p, pc) < distance:\n                return True\n        return False\n\n    close = []\n    for c in contours:\n        if is_close(point, c):\n            close.append(c)\n\n    return close\n\ndef merge_blobs(keypoints, min_overlap = 0.15):\n    \"\"\"Return a list of keypoints after merging those that overlap.\"\"\"\n\n    merge_pairs = []\n    merge_set = set()\n\n    for i, k1 in enumerate(keypoints):\n        for j, k2 in enumerate(keypoints):\n            if i >= j:\n                continue\n\n            p1, p2, r1, r2 = k1.pt, k2.pt, k1.size / 2, k2.size / 2\n\n            d = dist(p1, p2)\n\n            if d > r1 + r2:\n                continue\n\n            overlap = d / (r1 + r2)\n\n            if overlap > min_overlap:\n                merge_pairs.append((i, j))\n                merge_set.add(i)\n                merge_set.add(j)\n\n    if len(merge_pairs) == 0:\n        return keypoints\n\n    new_keypoints = []\n\n    for i, j in merge_pairs:\n        k1 = keypoints[i]\n        k2 = keypoints[j]\n\n        p1, p2, r1, r2 = k1.pt, k2.pt, k1.size / 2, k2.size / 2\n        d = dist(p1, p2)\n\n        r_ratio = r1 / (r1 + r2)\n\n        r_new = (r1 + r2 + d) / 2\n        x_new = p1[0] * r_ratio + p2[0] * (1 - r_ratio)\n        y_new = p1[1] * r_ratio + p2[1] * (1 - r_ratio)\n\n        new_keypoints.append(cv.KeyPoint(x_new, y_new, r_new * 2))\n\n    for i, k in enumerate(keypoints):\n        if i in merge_set:\n            continue\n\n        new_keypoints.append(k)\n\n    return new_keypoints\n\ndef get_closest_contour(point, contours):\n    \"\"\"Return the closest contour, given a point.\"\"\"\n    closest = None\n    closest_distance = float('inf')\n    for c in contours:\n        for pc in contour_to_list(c):\n            d = dist(point, pc)\n            if d < closest_distance:\n                closest_distance = d\n                closest = c\n\n    return closest\n\ndef point_to_line_distance(p1, p2, p3):\n    \"\"\"Return the distance from point p3 to a line defined by points p1 and p2.\"\"\"\n    p1 = np.array(p1)\n    p2 = np.array(p2)\n    p3 = np.array(p3)\n    return np.linalg.norm(np.cross(p2-p1, p1-p3)) / np.linalg.norm(p2-p1)\n\ndef point_to_segment_distance(a, b, p):\n    \"\"\"Return the distance from point p to a segment defined by points a and sb.\"\"\"\n    a = np.array(a)\n    b = np.array(b)\n    p = np.array(p)\n\n    # normalized tangent vector\n    d = np.divide(b - a, np.linalg.norm(b - a))\n\n    # signed parallel distance components\n    s = np.dot(a - p, d)\n    t = np.dot(p - b, d)\n\n    # clamped parallel distance\n    h = np.maximum.reduce([s, t, 0])\n\n    # perpendicular distance component\n    c = np.cross(p - a, d)\n\n    return np.hypot(h, np.linalg.norm(c))\n\n\ndef point_to_contour_distance(point, contour):\n    \"\"\"Return the distance from point to contour.\"\"\"\n    cl = contour_to_list(contour)\n\n    min_d = float('inf')\n    for i in range(len(cl) - 1):\n        d = point_to_segment_distance(cl[i], cl[i + 1], point)\n\n        if d < min_d:\n            min_d = d\n\n    return min_d\n\ndef squared_contour_error(contours_from, contour_to):\n    \"\"\"Return the average squared error of distances of points from a list of contours to another contour.\"\"\"\n    point_count = 0\n    contour_error = 0\n\n    for c in contours_from:\n        point_count += len(c)\n\n        for pc in contour_to_list(c):\n            contour_error += point_to_contour_distance(pc, contour_to)\n\n    return contour_error / point_count\n\ndef detect_holds(img, keypoints, contours, threshold_step=5):\n    \"\"\"Detect holds by combining blob and edge detection.\"\"\"\n    # pre-compute thresholds and contours/edges since they're used repeatedly\n    blur = gaussian_blur(img)\n\n    thresholds = {}\n    for i in range(0, 255, threshold_step):\n        t = threshold(blur, i, 255)\n        thresholds[i] = []\n\n        for c_t in t[:,:,0], t[:,:,1], t[:,:,2]:\n            t_edges = canny(c_t)\n            t_contours = find_contours(t_edges)\n            t_contours = simplify_contours(t_contours)\n\n            thresholds[i].append((t_edges, t_contours))\n\n    hold_approximations = {}\n    for k in keypoints:\n        nearby_contours = get_nearby_contours(k.pt, contours, (k.size / 2))\n\n        if len(nearby_contours) == 0:\n            continue\n\n        best_contour = None\n        best_contour_error = float('inf')\n\n        # find optimal threshold\n        for i in range(0, 255, threshold_step):\n            for t_edges, t_contours in thresholds[i]:\n                closest_t_contour = get_closest_contour(k.pt, t_contours)\n\n                if closest_t_contour is None:\n                    continue\n\n                err = squared_contour_error(nearby_contours, closest_t_contour)\n\n                if best_contour_error > err:\n                    best_contour_error = err\n                    best_contour = closest_t_contour\n\n        if best_contour is None:\n            continue\n\n        hold_approximations[k] = best_contour\n\n    return hold_approximations\n\ndef to_detectron_format(img, contours):\n    \"\"\"Convert the contours of holds to a format that is parsable by detectron.\n    https://detectron2.readthedocs.io/en/latest/tutorials/models.html#model-output-format\"\"\"\n    h, w = img.shape[:2]\n\n    instances = Instances((h, w))\n\n    boxes = []\n    for c in contours:\n        boxes.append(contour_to_box(c))\n\n    masks = []\n    for c in contours:\n        masks.append(contour_to_mask(img, c))\n\n    instances.set(\"pred_boxes\", Boxes(torch.tensor(boxes)))\n    instances.set(\"pred_masks\", torch.tensor(masks))\n\n    return instances\n\ndef find_hold_contours(image, d_dict):\n    \"\"\"find hold contours\"\"\"\n    gray = cv.cvtColor(image, cv.COLOR_BGR2GRAY)\n    points = np.array([[0,0],[d_dict[\"width\"]-1,0],[d_dict[\"width\"]-1,d_dict[\"height\"]-1],[0,d_dict[\"height\"]-1]])\n\n    cv.fillPoly(gray,np.int32([points]),(0,0,0))\n    for poly in d_dict[\"annotations\"]:\n        p=np.array(poly[\"segmentation\"])\n        points = np.reshape(p,[int(p.shape[1]/2),2])\n        cv.fillPoly(gray,np.int32([points]),(255,255,255))\n    mask = gray\n\n    contours, _ = cv.findContours(mask, cv.RETR_EXTERNAL, cv.CHAIN_APPROX_SIMPLE)\n    return contours\n\ndef draw_segmented_objects(image, contours, label_cnt_idx, holds_count):\n    mask = np.zeros_like(image[:, :, 0])\n    cv.drawContours(mask, [contours[i] for i in label_cnt_idx], -1, (255), -1)\n    masked_image = cv.bitwise_and(image, image, mask=mask)\n    masked_image = cv.putText(masked_image, f'{holds_count} holds', (200, 1200), cv.FONT_HERSHEY_SIMPLEX,\n                        fontScale = 7, color = (255, 255, 255), thickness = 12, lineType = cv.LINE_AA)\n    return masked_image\n\ndef plot_result(image, contours, df_mean_color):\n    img = image.copy()\n    for label, df_grouped in df_mean_color.groupby('label'):\n        holds_amount = len(df_grouped)\n        masked_image = draw_segmented_objects(image, contours, df_grouped.index, holds_amount)\n        img = cv.hconcat([img, masked_image])\n\n    plt.imshow(cv.cvtColor(img, cv.COLOR_BGR2RGB) )\n    plt.axis('off')\n\ndef plot_routes(routes: List, d_dict):\n    image_bgr = cv.imread(d_dict[\"file_name\"])\n    for k, holds in routes.items():\n        height = d_dict[\"height\"]\n        width = d_dict[\"width\"]\n        final_bitmask = np.zeros((height, width))\n        for hold_id in holds:\n            hold_poly = d_dict[\"annotations\"][hold_id]\n            points = np.array(hold_poly[\"segmentation\"])\n            points = points.reshape((-1, 2)).astype(np.int32)\n            points = points[np.newaxis, :]\n            bitmask = cv.fillPoly(np.zeros(image_bgr.shape[:2]), points, 255).astype(\n                np.uint8\n            )\n            final_bitmask = final_bitmask + bitmask\n        final_bitmask = final_bitmask.astype(np.bool8).astype(np.float32)\n        img = image_bgr.transpose((2, 0, 1))\n        masked_output = img * final_bitmask\n        img_routes = masked_output.astype(np.int32).transpose((1, 2, 0))\n        fig, ax = plt.subplots(ncols=2)\n        ax[0].imshow(img_routes[:, :, ::-1])\n        ax[1].imshow(image_bgr[:, :, ::-1])\n\n# Masked versions of color moments https://en.wikipedia.org/wiki/Color_moments\ndef masked_skewness(image, mask):\n    np_image = np.transpose(image, (2, 0, 1)).astype(\"float64\")\n    mask_3 = np.array([mask, mask, mask])\n    np_mask = np.ma.make_mask(mask_3)\n    np_image = np.ma.array(np_image, mask=np_mask)\n    image_raveled = np.ma.reshape(np_image, (3, -1))\n    return skew(image_raveled, axis=1)\n\n# Masked versions of color moments https://en.wikipedia.org/wiki/Color_moments\ndef masked_kurtosis(image, mask):\n    np_image = np.transpose(image, (2, 0, 1)).astype(\"float64\")\n    mask_3 = np.array([mask, mask, mask])\n    np_mask = np.ma.make_mask(mask_3)\n    np_image = np.ma.array(np_image, mask=np_mask)\n    image_raveled = np.ma.reshape(np_image, (3, -1))\n    return kurtosis(image_raveled, axis=1)\n\ndef get_color_moments(image, d_dict):\n    color_moments = defaultdict(list)\n    for idx, poly in enumerate(d_dict[\"annotations\"]):\n        if poly[\"category_id\"] == 1:\n        # Volumes are considered part of the wall, not part of a route -> skip\n            continue\n        points = np.array(poly[\"segmentation\"])\n        points = points.reshape((-1, 2)).astype(np.int32)\n        points = points[np.newaxis, :]\n        mask = cv.fillPoly(np.zeros(image.shape[:2]), points, 255).astype(np.uint8)\n        mean_v, std_v = cv.meanStdDev(image, mask=mask)\n        # skew_v = masked_skewness(image, mask)\n        # kurtosis_v = masked_kurtosis(image, mask)\n        color_moments[idx].extend(mean_v[0:1, 0])\n        color_moments[idx].extend(std_v[0:1, 0])\n        # color_moments[idx].extend(skew_v[0:3])\n        # color_moments[idx].extend(kurtosis_v[0:3])\n\n    color_moment_arr = np.array(list(color_moments.values()))\n    color_moments_arr_norm = color_moment_arr - np.mean(color_moment_arr, axis=0)\n    color_moments_arr_norm = color_moments_arr_norm / np.std(color_moments_arr_norm, axis=0)\n    return color_moments_arr_norm\n\ndef get_histograms(image, d_dict):\n    histograms = defaultdict(list)\n    for idx, poly in enumerate(d_dict[\"annotations\"]):\n        if poly[\"category_id\"] == 1:\n        # Volumes are considered part of the wall, not part of a route -> skip\n            continue\n        points = np.array(poly[\"segmentation\"])\n        points = points.reshape((-1, 2)).astype(np.int32)\n        points = points[np.newaxis, :]\n        mask = cv.fillPoly(np.zeros(image.shape[:2]), points, 255).astype(np.uint8)\n        hist0 = cv.calcHist([image], [0], mask=mask, histSize=[256], ranges=[0, 256])\n        hist1 = cv.calcHist([image], [1], mask=mask, histSize=[256], ranges=[0, 256])\n        hist2 = cv.calcHist([image], [2], mask=mask, histSize=[256], ranges=[0, 256])\n        hist = np.array([hist0])\n        histograms[idx] = hist.flatten()\n\n    histograms_arr = np.array(list(histograms.values()))\n    histograms_arr_norm = histograms_arr - np.mean(histograms_arr, axis=0)\n    histograms_arr_norm = np.nan_to_num(\n    histograms_arr_norm / np.std(histograms_arr_norm, axis=0)\n    )\n    return histograms_arr_norm","metadata":{"execution":{"iopub.status.busy":"2024-10-07T22:53:02.877861Z","iopub.execute_input":"2024-10-07T22:53:02.878754Z","iopub.status.idle":"2024-10-07T22:53:02.967388Z","shell.execute_reply.started":"2024-10-07T22:53:02.878687Z","shell.execute_reply":"2024-10-07T22:53:02.965712Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#from std.utils import *\nfrom matplotlib import pyplot as plt\n\n\nimg = cv.imread('/kaggle/input/indoor-climbing-gym-hold-segmentation/sm/001.jpg')\n\n# blob detection\nkeypoints = detect_blobs(img)\nimg_blobs = img.copy()\ndraw_keypoints(img_blobs, keypoints)\n\n# merge blobs that have an overlapping\nkeypoints = merge_blobs(keypoints)\nimg_blobs_merged = img.copy()\ndraw_keypoints(img_blobs_merged, keypoints)                                                     \n\n\n# display the results\nfig, (ax1, ax2, ax3) = plt.subplots(1, 3)\nax1.imshow(img[:, :, ::-1])\nax1.axis('off')\nax1.set_title('Original')\n\nax2.imshow(img_blobs[:, :, ::-1])\nax2.axis('off')\nax2.set_title('Raw blobs')\n\nax3.imshow(img_blobs_merged[:, :, ::-1])\nax3.axis('off')\nax3.set_title('Merged blobs')\n\nfig.tight_layout()\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2024-10-07T22:56:18.38773Z","iopub.execute_input":"2024-10-07T22:56:18.388175Z","iopub.status.idle":"2024-10-07T22:56:21.798485Z","shell.execute_reply.started":"2024-10-07T22:56:18.38813Z","shell.execute_reply":"2024-10-07T22:56:21.797236Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# blur the image for smoother edges\nblur = gaussian_blur(img)\n\n# find the contours\nedges = canny(blur)\ncontours = find_contours(edges)\n\nimg_contours = img.copy()\ndraw_contours(img_contours, contours)\n\ncontours = filter_size_contours(contours)\nimg_contours_area = img.copy()\ndraw_contours(img_contours_area, contours)\n\ncontours = filter_straight_contours(contours)\nimg_contours_straight = img.copy()\ndraw_contours(img_contours_straight, contours)\n\n\n# display the results\nfig, ((ax1, ax2), (ax3, ax4)) = plt.subplots(2, 2)\nax1.imshow(img[:, :, ::-1])\nax1.axis('off')\nax1.set_title('Original')\n\nax2.imshow(img_contours[:, :, ::-1])\nax2.axis('off')\nax2.set_title('Raw contours')\n\nax3.imshow(img_contours_area[:, :, ::-1])\nax3.axis('off')\nax3.set_title('Area-filtered contours')\n\nax4.imshow(img_contours_straight[:, :, ::-1])\nax4.axis('off')\nax4.set_title('Straightness-filtered contours')\n\nfig.tight_layout()\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2024-10-07T23:00:48.733015Z","iopub.execute_input":"2024-10-07T23:00:48.733661Z","iopub.status.idle":"2024-10-07T23:00:53.182087Z","shell.execute_reply.started":"2024-10-07T23:00:48.733612Z","shell.execute_reply":"2024-10-07T23:00:53.180743Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# contours are simplified to speed up the detection\ncontours = simplify_contours(contours)\n\n# approximate holds by their contours\n# returns a dictionary of keypoint-contour\n# NOTE that this step can take a few minutes, depending on a number of keypoints\nhold_approximations = detect_holds(img, keypoints, contours)\n\nimg_holds = img.copy()\ndraw_contour_boxes(img_holds, list(hold_approximations.values()), color=(0, 255, 0))\ndraw_contours(img_holds, list(hold_approximations.values()), color=(0, 128, 0))\n\n\n# display the results\nfig, (ax1, ax2) = plt.subplots(1, 2)\nax1.imshow(img[:, :, ::-1])\nax1.axis('off')\nax1.set_title('Original')\n\nax2.imshow(img_holds[:, :, ::-1])\nax2.axis('off')\nax2.set_title('Detected holds')\n\nfig.tight_layout()\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2024-10-07T23:01:44.827995Z","iopub.execute_input":"2024-10-07T23:01:44.82904Z","iopub.status.idle":"2024-10-07T23:05:18.481415Z","shell.execute_reply.started":"2024-10-07T23:01:44.828993Z","shell.execute_reply":"2024-10-07T23:05:18.480182Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import os\nimport cv2\nimport matplotlib.pyplot as plt\nfrom detectron2.config import get_cfg\nfrom detectron2.engine import DefaultPredictor\nfrom detectron2.utils.visualizer import Visualizer\nfrom detectron2.data import MetadataCatalog\n\nMODEL_DIRECTORY = \"/kaggle/input/indoor-climbing-gym-hold-segmentation/model/\"\nSAMPLE_IMAGE = \"/kaggle/input/indoor-climbing-gym-hold-segmentation/sm/001.jpg\"\n\n# Get config and weigths for model\ncfg = get_cfg()\ncfg.merge_from_file(os.path.join(MODEL_DIRECTORY, \"experiment_config.yml\"))\ncfg.MODEL.WEIGHTS = os.path.join(MODEL_DIRECTORY, \"model_final.pth\")\ncfg.MODEL.DEVICE='cpu'\n\n# Set metadata, in this case only the class names for plotting\nMetadataCatalog.get(\"meta\").thing_classes = [\"hold\", \"volume\"]\nmetadata = MetadataCatalog.get(\"meta\")\n\npredictor = DefaultPredictor(cfg)\n\nimg = cv2.imread(SAMPLE_IMAGE)\noutputs = predictor(img)\nv = Visualizer(\n    img[:, :, ::-1],\n    metadata=metadata\n)\n\nout_predictions = v.draw_instance_predictions(outputs[\"instances\"].to(\"cpu\"))\nimg_holds = out_predictions.get_image()\n# display the results\nfig, (ax1, ax2) = plt.subplots(1, 2)\nax1.imshow(img[:, :, ::-1])\nax1.axis('off')\nax1.set_title('Original')\n\nax2.imshow(img_holds)\nax2.axis('off')\nax2.set_title('Detected holds')\n\nfig.tight_layout()\nplt.savefig('/kaggle/working/holds_plot.jpg')\nprint(os.listdir('/kaggle/working'))\nplt.show()\n\n","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2024-10-07T23:11:59.850623Z","iopub.execute_input":"2024-10-07T23:11:59.851099Z","iopub.status.idle":"2024-10-07T23:12:13.4479Z","shell.execute_reply.started":"2024-10-07T23:11:59.851059Z","shell.execute_reply":"2024-10-07T23:12:13.446625Z"},"trusted":true},"execution_count":null,"outputs":[]}]}